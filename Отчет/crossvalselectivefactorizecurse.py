# -*- coding: utf-8 -*-
"""CrossValSelectiveFactorizeCurse.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1umIt4sXdwWXPWhqDlLD6jqngmj4worqH
"""

import tensorflow as tf
tf.random.set_seed(42)

!pip install swifter

import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, accuracy_score, roc_curve
from keras.layers import Dense, Dropout, Input, BatchNormalization
from keras.models import Sequential, Model
from keras.regularizers import l2 # L2-regularisation
import gc
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from keras import callbacks
import swifter
import h5py
from tensorflow.keras.utils import Sequence
from sklearn.ensemble import ExtraTreesClassifier
import matplotlib.pyplot as plt
from sklearn.feature_extraction import FeatureHasher
from sklearn.model_selection import StratifiedKFold

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/ML/DA/train.csv.zip', dtype = {'id':'str'})
df

df.dtypes

colls = df.iloc[:, 1:-1].columns.tolist()
for coll in colls:
    print(df[coll].value_counts())
    print('\n\n')

"""#Исследование строковых признаков"""

colls = df.columns.tolist()
collMinMax = {}
for col in colls:
    collMinMax[col] = (df[col].dropna().min(), df[col].dropna().max())
collMinMax

"""Есть подозрения, что признаки с nom_5 по nom_9 являются числами в 16-ой системе исчисления, давайте преобразуем их в десятичные числа"""

def get_decimal(hex):
    return (int(hex, 16) if(not pd.isna(hex)) else hex)
    # if(pd.isna(hex)):
    #     return hex
    # return l.index(hex)

colls = ['nom_5',
        'nom_6',
        'nom_7',
        'nom_8',
        'nom_9']
for col in colls:
    df[col] = df[col].swifter.apply(lambda x: get_decimal(x))

df.describe()

"""Проверим данные на пропуски"""

colls = df.columns.tolist()
isNA = {}
for col in colls:
    isNA[col] = any(df[col].isna())
isNA

"""Пропуски присутстсуют у каждого признака, кроме идентификатора и таргета"""

print('Количество строк без имзенений ', df.shape[0])
print('Количество строк после удаления всех пропусков ', df.dropna().shape[0])

"""Почти половина данных теряется. Рассмотрим целостноть каждого признака в частности"""

naColls = {k:v for k,v in isNA.items() if (v == True)}
naColls

colls = df.columns.tolist()
dropNaCount = {}
for col in naColls:
    dropNaCount[col] = round(df[col].dropna().shape[0] / df.shape[0] * 100, 2)
dropNaCount

"""Потери для каждого признака в частности незначительные, но видимо, относительно всей выборки, распределение пропусков построчно близко к равномерному и 3-4% пропусков каждого признака делает половину данных зашумленными.

Для бинарных признаков и порядковых признаков месяца и дня введем заполнение пропусков через равномерное распределение.

Для остальных категорий для заполнения пропусков отведем отдельную категорию



Для образование новой категории в каждом признаке возьмем его максимальное значение и увеличим на еденицу
"""

def get_filled_na(df):
    bincolls = ['bin_0', 'bin_1', 'bin_2', 'day', 'month', 'ord_0']
    for coll in bincolls:
        min_ = df[coll].min()
        max_ = df[coll].max()
        df[coll] = df[coll].swifter.apply(lambda x: round(np.random.uniform(min_, max_)) if(np.isnan(x)) else x)

    serialcolls = ['nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']
    for coll in serialcolls:
        max_ = df[coll].max()
        df[coll] = df[coll].swifter.apply(lambda x: (max_ + 1) if(np.isnan(x)) else x)
        df[coll] = df[coll].factorize(sort = True)[0] 
    
    textserialColls = ['bin_3', 'bin_4', 'ord_3', 	'ord_4', 'ord_5', 'nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4']
    for coll in textserialColls:
        uniq = df[coll].dropna().unique().tolist()
        n = len(uniq)
        df[coll] = df[coll].swifter.apply(lambda x: uniq[round(np.random.uniform(0, n-1))] if(pd.isna(x)) else x)
        df[coll] = df[coll].factorize(sort = True)[0] 

    textserialColls = ['ord_1', 'ord_2']
    for coll in textserialColls:
        uniq = df[coll].dropna().unique().tolist()
        n = len(uniq)
        df[coll] = df[coll].swifter.apply(lambda x: uniq[round(np.random.uniform(0, n-1))] if(pd.isna(x)) else x)
    
    cat = {'Novice':0, 'Contributor':1, 'Expert':2, 'Master':3, 'Grandmaster':4}
    df['ord_1'] = df['ord_1'].swifter.apply(lambda x: cat[x])
 
           
        
    cat = {'Freezing':0, 'Cold':1, 'Warm':2, 'Hot':3, 'Boiling Hot':4, 'Lava Hot':5}
    df['ord_2'] = df['ord_2'].swifter.apply(lambda x: cat[x])
    
    return df

df = get_filled_na(df)

display(df.describe())
print(df.shape)
print(df.columns.tolist())

colls = df.columns.tolist()
dropNaCount = {}
for col in colls:
    dropNaCount[col] = round(df[col].dropna().shape[0] / df.shape[0] * 100, 2)
dropNaCount

"""Все пропуски восполнены

# BaseLine

Так как распределение target меток сильно смещено в сторону значения 0, просто сделаем все предсказанные ответы равные нулю
"""

y_pred_baseline = [0] * df.shape[0]
print('Наш baseline ROC AUC:', round(roc_auc_score(df.target.values.tolist(), y_pred_baseline), 2))

"""# Экспериментальная установка

Так как мы не можем оценить линейность или нелинейность признаков из-за большой размерности, возьмем за экспериментальную установку 5 сверточных слоев с классификатором на выходном слое и методами регуляризации после каждой свертки. Этого должно более чем хватить для исслоедования категориального кодирования.
"""

def get_model(num_vars, num_out):
    units = 512
    l2_lambda = 0.0001
    inputs = Input(shape=(num_vars,))
    #1
    x = BatchNormalization()(inputs)
    x = Dense(units, activation = 'relu', kernel_regularizer=l2(l2_lambda),
                                          bias_regularizer= l2(l2_lambda),
                                          activity_regularizer = l2(l2_lambda))(x)
    x = Dropout(0.25)(x)
    #2
    x = Dense(units, activation = 'relu', kernel_regularizer=l2(l2_lambda),
                                          bias_regularizer= l2(l2_lambda),
                                          activity_regularizer = l2(l2_lambda))(x)
    x = Dropout(0.25)(x)
    #3
    x = Dense(units, activation = 'relu', kernel_regularizer=l2(l2_lambda),
                                          bias_regularizer= l2(l2_lambda),
                                          activity_regularizer = l2(l2_lambda))(x)
    x = Dropout(0.25)(x)
    #4
    x = Dense(units, activation = 'relu', kernel_regularizer=l2(l2_lambda),
                                          bias_regularizer= l2(l2_lambda),
                                          activity_regularizer = l2(l2_lambda))(x)
    x = Dropout(0.25)(x)
    #5
    x = Dense(units, activation = 'relu', kernel_regularizer=l2(l2_lambda),
                                          bias_regularizer= l2(l2_lambda),
                                          activity_regularizer = l2(l2_lambda))(x)
    x = Dropout(0.25)(x)

    x = Dense(num_out, activation = 'sigmoid', kernel_regularizer=l2(l2_lambda),
                                          bias_regularizer= l2(l2_lambda),
                                          activity_regularizer = l2(l2_lambda))(x)
    return Model(inputs=inputs, outputs=x)

def get_X_Y(df):
    X = df.iloc[:, 1:-1]
    gc.collect()
    Y = df.target
    gc.collect()
    return X, Y


from sklearn.ensemble import ExtraTreesClassifier

def hashing_features(X, Y, n):
    h = FeatureHasher(n_features= n)
    X_fi = h.fit_transform(X.drop(['nom_5' ,'nom_6' ,'nom_7' ,'nom_8', 'nom_9', 'ord_5', 'ord_3', 'ord_4'], axis = 1).to_dict(orient = 'records'))
    X_fi = pd.DataFrame(X_fi.toarray()).join(X[['nom_5' ,'nom_6' ,'nom_7' ,'nom_8', 'nom_9', 'ord_5', 'ord_3', 'ord_4']])
    return X_fi,Y


def OneHotEncoding(X, Y):
    X_fi = X.drop(['nom_5' ,'nom_6' ,'nom_7' ,'nom_8', 'nom_9', 'ord_5', 'ord_3', 'ord_4'], axis = 1)
    X_ = pd.DataFrame()
    colls = X_fi.columns.tolist()
    for coll in colls:
        gc.collect()
        X_ = pd.concat([X_, pd.get_dummies(X_fi[coll])], axis = 1)
        gc.collect()
    X_fi = pd.concat([X_, X[['nom_5' ,'nom_6' ,'nom_7' ,'nom_8', 'nom_9', 'ord_5', 'ord_3', 'ord_4']]], axis = 1)
    return X_fi,Y


def get_factorize_X_Y(df):
    X = get_factorize_df(df.iloc[:, 1:-1])
    Y = df.target
    return X, Y

def get_dummies_X_Y(df):
    X = pd.get_dummies(df.iloc[:, 1:-1])
    Y = df.target
    return X, Y

gc.collect()
X, Y = get_X_Y(df)
gc.collect()

X.head()

import seaborn as sns
fig, ax = plt.subplots(figsize = (15, 15))
d = X.join(df.target).corr()
ax = sns.heatmap(d, vmin = -0.4, vmax = 0.4, cmap = 'hsv')

display(X.head())
print(X.shape)

X_train, X_val, Y_train, Y_val = train_test_split(X, Y,  test_size = 0.25,  random_state=42)
gc.collect()

print('Распределение тестовых меток\n', dict(Y_val.value_counts()))
print('Распределение тренировочных меток\n',dict(Y_train.value_counts()))

display(X_train.describe)
display(Y_train.describe)
print(X_val.columns.tolist())
display(X_val.describe)
display(Y_val.describe)
gc.collect()

"""Классы распределены более менее равномерно"""

X = (X - X.mean()) / X.std()

# количество эпох\итераций для обучения


model = get_model(X_train.shape[1], 1)

model.compile(loss='categorical_crossentropy',
              optimizer='rmsprop',
              metrics= [tf.keras.metrics.AUC(), 'accuracy']
            )

print(model.summary())

gc.collect()

class DummiesSequenceTrain(Sequence):

    def __init__(self, batch_size):
        self.batch_size = batch_size

    def __len__(self):
        return int(np.floor(X_train.shape[0] / self.batch_size))

    def __getitem__(self, idx):
        batch_x = X_train.iloc[idx * self.batch_size:(idx + 1) *
                                self.batch_size].values
        batch_y = Y_train.iloc[idx * self.batch_size:(idx + 1) *
                                self.batch_size].values

        return batch_x, batch_y

class DummiesSequenceValid(Sequence):

    def __init__(self, batch_size):
        self.batch_size = batch_size

    def __len__(self):
        return int(np.floor(X_val.shape[0] / self.batch_size))

    def __getitem__(self, idx):
        batch_x = X_val.iloc[idx * self.batch_size:(idx + 1) *
                                self.batch_size].values
        batch_y = Y_val.iloc[idx * self.batch_size:(idx + 1) *
                                self.batch_size].values

        return batch_x, batch_y

batch = 8192 
train_gen = DummiesSequenceTrain( int(batch))
val_gen = DummiesSequenceValid( int(batch))

class GarbageCollectorCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        gc.collect()

    def on_epoch_start(self, epoch, logs=None):
        gc.collect()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# path =  '/content/drive/MyDrive/ML/DA/WeightsDA/checkpoint_NN'
# callbacks = [
#                 tf.keras.callbacks.EarlyStopping(patience=5, monitor='loss'), GarbageCollectorCallback(),
#                 tf.keras.callbacks.EarlyStopping(patience=5, monitor='val_loss'), GarbageCollectorCallback(),
#                 tf.keras.callbacks.ModelCheckpoint(filepath = path
#                                                   , monitor='val_loss'
#                                                   , verbose=1
#                                                    , save_best_only= True
#                                                    , save_weights_only= True
#                                                    , mode = 'min')
#             ]
# epochs = 50
#

# gc.collect()
# kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
# model = XGBClassifier(verbosity = 1)

# results = cross_val_score(model, X.values, Y.values, cv=kfold, scoring = 'roc_auc', verbose = 1)
# print("ROC AUC: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100))



kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cvscores = []
for train, val in kfold.split(X, df.target):
        class DummiesSequenceTrain(Sequence):
            def __init__(self, batch_size):
                self.batch_size = batch_size

            def __len__(self):
                return int(np.floor(len(X.values[train]) / self.batch_size))

            def __getitem__(self, idx):
                batch_x = (X.values[train])[idx * self.batch_size:(idx + 1) *
                                        self.batch_size]
                batch_y = (Y.values[train])[idx * self.batch_size:(idx + 1) *
                                        self.batch_size]

                return batch_x, batch_y


        class DummiesSequenceValid(Sequence):
            def __init__(self, batch_size):
                self.batch_size = batch_size

            def __len__(self):
                return int(np.floor(len(X.values[val])/ self.batch_size))

            def __getitem__(self, idx):
                batch_x = (X.values[val])[idx * self.batch_size:(idx + 1) *
                                        self.batch_size]
                batch_y = (Y.values[val])[idx * self.batch_size:(idx + 1) *
                                        self.batch_size]

                return batch_x, batch_y

        train_gen = DummiesSequenceTrain(int(batch))
        val_gen = DummiesSequenceValid(int(batch))
        model = get_model(X_train.shape[1], 1)

        model.compile(loss='binary_crossentropy',
                    optimizer='rmsprop',
                    metrics= [tf.keras.metrics.AUC(), 'accuracy']
                    )
        history = model.fit(x = train_gen
                          , epochs=epochs
                          , callbacks=callbacks
                          , validation_data = val_gen )
        # evaluate the model
        scores = model.evaluate(x = val_gen, verbose=0)
        print(model.metrics_names,'\n', scores)
        cvscores.append(scores[1])
print("%.2f%% (+/- %.2f%%)" % (np.mean(cvscores), np.std(cvscores)))

print("%.6f (+/- %.6f)" % (np.mean(cvscores), np.std(cvscores)))

history = history.history

import matplotlib.pyplot as plt

fig, ax = plt.subplots(1, 2, figsize = (30, 10))



ax[0].plot(history['loss'], label = 'Loss')
ax[1].plot(history['auc'], label = 'ROC AUC')
ax[0].plot(history['val_loss'], label = 'Validation Loss')
ax[0].grid('both')
ax[0].legend()
ax[0].set_xlabel('Эпохи')
ax[0].set_ylabel('Функция потерь')
ax[1].plot(history['val_auc'], label = 'Validation ROC AUC')
ax[1].legend()
ax[1].grid('both')
ax[1].set_xlabel('Эпохи')
ax[1].set_ylabel('ROC AUC')

import matplotlib.pyplot as plt
fig, ax = plt.subplots(1, 2, figsize = (30, 10))



ax[0].plot(history['loss'], label = 'Loss')
ax[1].plot(history['auc_7'], label = 'ROC AUC')
ax[0].plot(history['val_loss'], label = 'Validation Loss')
ax[0].grid('both')
ax[0].legend()
ax[0].set_xlabel('Эпохи')
ax[0].set_ylabel('Функция потерь')
ax[1].plot(history['val_auc_7'], label = 'Validation ROC AUC')
ax[1].legend()
ax[1].grid('both')
ax[1].set_xlabel('Эпохи')
ax[1].set_ylabel('ROC AUC')
plt.savefig('/content/drive/MyDrive/ML/DA/CrossValSelective_Encoding_OHE_X.png')

"""# Self Sumbission"""

model.load_weights(path)
results = model.predict(X)
#results = np.argmax(results,axis = 1)

results_ = np.round(results)
print('ROC AUC', roc_auc_score(pd.get_dummies(df.target), results_, average=None))
print('Accuracy', accuracy_score(pd.get_dummies(df.target), results_))

#results_ = np.argmax(results,axis = 1)
# print('ROC AUC', roc_auc_score(df.target, results_))
# print('Accuracy', accuracy_score(df.target, results_))

results

"""# Submission"""

model.load_weights(path)

df_test = pd.read_csv('/content/drive/MyDrive/ML/DA/test.zip',  dtype = {'id':'str'})
display(df_test.head())
print(df_test.shape)

def get_decimal(hex):
    return (int(hex, 16) if(not pd.isna(hex)) else hex)
    # if(pd.isna(hex)):
    #     return hex
    # return l.index(hex)

colls = ['nom_5',
        'nom_6',
        'nom_7',
        'nom_8',
        'nom_9']
for col in colls:
    df_test[col] = df_test[col].swifter.apply(lambda x: get_decimal(x))

df_test = get_filled_na(df_test)

df_test.describe().shape

gc.collect()
X_test, Y  = OneHotEncoding(df_test.iloc[:, 1:], pd.DataFrame())

Y_pred = model.predict(X_test)

Y_test = list(map(np.argmax, Y_pred))
Y_test

Y_test = pd.DataFrame({'id':df_test.id,
                       'target':Y_test})
display(Y_test.describe())
print(Y_test.shape)

Y_test.head()

Y_test.to_csv('/content/drive/MyDrive/ML/DA/OHE_submission.csv', index = False)

Y_test.target.value_counts()

